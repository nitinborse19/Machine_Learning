{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>398.799988</td>\n",
       "      <td>399.359985</td>\n",
       "      <td>394.019989</td>\n",
       "      <td>397.970001</td>\n",
       "      <td>397.970001</td>\n",
       "      <td>2137800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>398.290009</td>\n",
       "      <td>402.709991</td>\n",
       "      <td>396.220001</td>\n",
       "      <td>396.440002</td>\n",
       "      <td>396.440002</td>\n",
       "      <td>2210200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>395.850006</td>\n",
       "      <td>397.000000</td>\n",
       "      <td>388.420013</td>\n",
       "      <td>393.630005</td>\n",
       "      <td>393.630005</td>\n",
       "      <td>3170600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-07</th>\n",
       "      <td>395.040009</td>\n",
       "      <td>398.470001</td>\n",
       "      <td>394.290009</td>\n",
       "      <td>398.029999</td>\n",
       "      <td>398.029999</td>\n",
       "      <td>1916000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-08</th>\n",
       "      <td>398.470001</td>\n",
       "      <td>403.000000</td>\n",
       "      <td>396.040009</td>\n",
       "      <td>401.920013</td>\n",
       "      <td>401.920013</td>\n",
       "      <td>2316500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2014-01-02  398.799988  399.359985  394.019989  397.970001  397.970001   \n",
       "2014-01-03  398.290009  402.709991  396.220001  396.440002  396.440002   \n",
       "2014-01-06  395.850006  397.000000  388.420013  393.630005  393.630005   \n",
       "2014-01-07  395.040009  398.470001  394.290009  398.029999  398.029999   \n",
       "2014-01-08  398.470001  403.000000  396.040009  401.920013  401.920013   \n",
       "\n",
       "             Volume  \n",
       "Date                 \n",
       "2014-01-02  2137800  \n",
       "2014-01-03  2210200  \n",
       "2014-01-06  3170600  \n",
       "2014-01-07  1916000  \n",
       "2014-01-08  2316500  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=pd.read_csv(\"C:\\\\Users\\\\Krishna\\\\Desktop\\\\dataset\\\\AMZNtrain.csv\",parse_dates=[\"Date\"],index_col=\"Date\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain=train[[\"Open\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-01-02</th>\n",
       "      <td>398.799988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-03</th>\n",
       "      <td>398.290009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-06</th>\n",
       "      <td>395.850006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-07</th>\n",
       "      <td>395.040009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-08</th>\n",
       "      <td>398.470001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-24</th>\n",
       "      <td>1346.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-26</th>\n",
       "      <td>1368.890015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-27</th>\n",
       "      <td>1454.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-28</th>\n",
       "      <td>1473.349976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-12-31</th>\n",
       "      <td>1510.800049</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open\n",
       "Date                   \n",
       "2014-01-02   398.799988\n",
       "2014-01-03   398.290009\n",
       "2014-01-06   395.850006\n",
       "2014-01-07   395.040009\n",
       "2014-01-08   398.470001\n",
       "...                 ...\n",
       "2018-12-24  1346.000000\n",
       "2018-12-26  1368.890015\n",
       "2018-12-27  1454.199951\n",
       "2018-12-28  1473.349976\n",
       "2018-12-31  1510.800049\n",
       "\n",
       "[1258 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM are sensitive to the scale of the data. so we apply MinMax scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc=MinMaxScaler()\n",
    "sc_xtrain=sc.fit_transform(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06523313],\n",
       "       [0.06494233],\n",
       "       [0.06355099],\n",
       "       ...,\n",
       "       [0.66704299],\n",
       "       [0.67796271],\n",
       "       [0.69931748]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1258"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sc_xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert an array of values into a dataset matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform(n_steps,data):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(n_steps,len(data)):\n",
    "        x.append(data[i-n_steps:i,0])\n",
    "        y.append(data[i,0])\n",
    "    return np.array(x),np.array(y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,y_train=feature_transform(n_steps,sc_xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1246, 12), (1246,))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape((x_train.shape[0],x_train.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Stacked LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM\n",
    "model=Sequential()\n",
    "model.add(LSTM(50,activation=\"relu\",input_shape=(n_steps,1)))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50)                10400     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 10,451\n",
      "Trainable params: 10,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "42/42 [==============================] - 3s 8ms/step - loss: 0.1162\n",
      "Epoch 2/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.0026\n",
      "Epoch 3/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 7.7907e-04\n",
      "Epoch 4/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 4.0893e-04\n",
      "Epoch 5/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 4.2398e-04\n",
      "Epoch 6/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 3.6704e-04\n",
      "Epoch 7/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 3.7930e-04\n",
      "Epoch 8/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 3.8603e-04\n",
      "Epoch 9/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 3.4862e-04\n",
      "Epoch 10/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 3.6528e-04\n",
      "Epoch 11/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 3.9412e-04\n",
      "Epoch 12/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 3.8170e-04\n",
      "Epoch 13/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 3.3040e-04\n",
      "Epoch 14/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 3.4300e-04\n",
      "Epoch 15/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 3.2841e-04\n",
      "Epoch 16/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 3.0098e-04\n",
      "Epoch 17/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.7677e-04\n",
      "Epoch 18/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.9078e-04\n",
      "Epoch 19/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 3.4252e-04\n",
      "Epoch 20/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 3.1853e-04\n",
      "Epoch 21/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 3.4494e-04\n",
      "Epoch 22/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 3.8864e-04\n",
      "Epoch 23/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 3.1722e-04\n",
      "Epoch 24/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.7492e-04\n",
      "Epoch 25/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 3.6573e-04\n",
      "Epoch 26/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 3.0223e-04\n",
      "Epoch 27/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 2.9052e-04\n",
      "Epoch 28/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 3.1174e-04\n",
      "Epoch 29/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.8667e-04\n",
      "Epoch 30/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 2.6739e-04\n",
      "Epoch 31/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.7487e-04\n",
      "Epoch 32/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.9983e-04\n",
      "Epoch 33/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.4118e-04\n",
      "Epoch 34/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.4910e-04\n",
      "Epoch 35/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 3.2499e-04\n",
      "Epoch 36/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.8276e-04\n",
      "Epoch 37/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 2.7151e-04\n",
      "Epoch 38/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.6712e-04\n",
      "Epoch 39/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.6368e-04\n",
      "Epoch 40/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.4238e-04\n",
      "Epoch 41/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 3.1918e-04\n",
      "Epoch 42/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.5325e-04\n",
      "Epoch 43/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.8536e-04\n",
      "Epoch 44/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 3.1009e-04\n",
      "Epoch 45/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 2.4028e-04\n",
      "Epoch 46/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 2.1067e-04\n",
      "Epoch 47/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 2.2881e-04\n",
      "Epoch 48/300\n",
      "42/42 [==============================] - ETA: 0s - loss: 2.7443e-0 - 0s 10ms/step - loss: 2.7086e-04\n",
      "Epoch 49/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.4987e-04\n",
      "Epoch 50/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.6779e-04\n",
      "Epoch 51/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.2925e-04\n",
      "Epoch 52/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.0674e-04\n",
      "Epoch 53/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.4303e-04\n",
      "Epoch 54/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.1525e-04\n",
      "Epoch 55/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.1769e-04\n",
      "Epoch 56/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 2.4773e-04\n",
      "Epoch 57/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 2.4156e-04\n",
      "Epoch 58/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.8358e-04\n",
      "Epoch 59/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.9747e-04\n",
      "Epoch 60/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.9589e-04\n",
      "Epoch 61/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 2.6058e-04\n",
      "Epoch 62/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.1552e-04\n",
      "Epoch 63/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.8762e-04\n",
      "Epoch 64/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.3256e-04\n",
      "Epoch 65/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.3945e-04\n",
      "Epoch 66/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.4344e-04\n",
      "Epoch 67/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.0838e-04\n",
      "Epoch 68/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.2818e-04\n",
      "Epoch 69/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.2323e-04\n",
      "Epoch 70/300\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 2.0972e-04\n",
      "Epoch 71/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.6945e-04\n",
      "Epoch 72/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.9411e-04\n",
      "Epoch 73/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.5336e-04\n",
      "Epoch 74/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.7524e-04\n",
      "Epoch 75/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.8180e-04\n",
      "Epoch 76/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.3287e-04\n",
      "Epoch 77/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.8463e-04\n",
      "Epoch 78/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.0047e-04\n",
      "Epoch 79/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.0235e-04\n",
      "Epoch 80/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.0292e-04\n",
      "Epoch 81/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.5557e-04\n",
      "Epoch 82/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.3256e-04\n",
      "Epoch 83/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 2.0248e-04\n",
      "Epoch 84/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.8936e-04\n",
      "Epoch 85/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 2.1180e-04\n",
      "Epoch 86/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 2.0478e-04\n",
      "Epoch 87/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.4753e-04\n",
      "Epoch 88/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.3411e-04\n",
      "Epoch 89/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.0244e-04\n",
      "Epoch 90/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.9315e-04\n",
      "Epoch 91/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8601e-04\n",
      "Epoch 92/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.0011e-04\n",
      "Epoch 93/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.7955e-04\n",
      "Epoch 94/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.8978e-04\n",
      "Epoch 95/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 2.3360e-04\n",
      "Epoch 96/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7904e-04\n",
      "Epoch 97/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.1870e-04\n",
      "Epoch 98/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.8436e-04\n",
      "Epoch 99/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.1821e-04\n",
      "Epoch 100/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 2.3493e-04\n",
      "Epoch 101/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.7802e-04\n",
      "Epoch 102/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.9993e-04\n",
      "Epoch 103/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6019e-04\n",
      "Epoch 104/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6548e-04\n",
      "Epoch 105/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.7350e-04\n",
      "Epoch 106/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8463e-04\n",
      "Epoch 107/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7902e-04\n",
      "Epoch 108/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8560e-04\n",
      "Epoch 109/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7968e-04\n",
      "Epoch 110/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8337e-04\n",
      "Epoch 111/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.0068e-04\n",
      "Epoch 112/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.1508e-04\n",
      "Epoch 113/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7561e-04\n",
      "Epoch 114/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.3598e-04\n",
      "Epoch 115/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 2.2772e-04\n",
      "Epoch 116/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8039e-04\n",
      "Epoch 117/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7700e-04\n",
      "Epoch 118/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.9895e-04\n",
      "Epoch 119/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5451e-04\n",
      "Epoch 120/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.6169e-04\n",
      "Epoch 121/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8205e-04\n",
      "Epoch 122/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5149e-04\n",
      "Epoch 123/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8340e-04\n",
      "Epoch 124/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.9108e-04\n",
      "Epoch 125/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4889e-04\n",
      "Epoch 126/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.9785e-04\n",
      "Epoch 127/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6103e-04\n",
      "Epoch 128/300\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 2.3417e-04\n",
      "Epoch 129/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8013e-04\n",
      "Epoch 130/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.9223e-04\n",
      "Epoch 131/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6244e-04\n",
      "Epoch 132/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.0471e-04\n",
      "Epoch 133/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6004e-04\n",
      "Epoch 134/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8839e-04\n",
      "Epoch 135/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.5903e-04\n",
      "Epoch 136/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8525e-04\n",
      "Epoch 137/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6102e-04\n",
      "Epoch 138/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.4946e-04\n",
      "Epoch 139/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.8563e-04\n",
      "Epoch 140/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6463e-04\n",
      "Epoch 141/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.6409e-04\n",
      "Epoch 142/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5783e-04\n",
      "Epoch 143/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8377e-04\n",
      "Epoch 144/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4461e-04\n",
      "Epoch 145/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6005e-04\n",
      "Epoch 146/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.3551e-04\n",
      "Epoch 147/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4765e-04\n",
      "Epoch 148/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8041e-04\n",
      "Epoch 149/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8914e-04\n",
      "Epoch 150/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7258e-04\n",
      "Epoch 151/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.2828e-04\n",
      "Epoch 152/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 2.1113e-04\n",
      "Epoch 153/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6414e-04\n",
      "Epoch 154/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4799e-04\n",
      "Epoch 155/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4857e-04\n",
      "Epoch 156/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4618e-04\n",
      "Epoch 157/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8598e-04\n",
      "Epoch 158/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6288e-04\n",
      "Epoch 159/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6337e-04\n",
      "Epoch 160/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.3706e-04\n",
      "Epoch 161/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6561e-04\n",
      "Epoch 162/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8642e-04\n",
      "Epoch 163/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6102e-04\n",
      "Epoch 164/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7123e-04\n",
      "Epoch 165/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7384e-04\n",
      "Epoch 166/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5365e-04\n",
      "Epoch 167/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.8672e-04\n",
      "Epoch 168/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5782e-04\n",
      "Epoch 169/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.3420e-04: 0s - loss: 1.2\n",
      "Epoch 170/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.2102e-04\n",
      "Epoch 171/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6270e-04\n",
      "Epoch 172/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.9077e-04\n",
      "Epoch 173/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6191e-04\n",
      "Epoch 174/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5988e-04\n",
      "Epoch 175/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5197e-04\n",
      "Epoch 176/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.2344e-04\n",
      "Epoch 177/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 2.3455e-04\n",
      "Epoch 178/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7700e-04\n",
      "Epoch 179/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.7170e-04\n",
      "Epoch 180/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 2.0928e-04\n",
      "Epoch 181/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.6506e-04\n",
      "Epoch 182/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.3563e-04\n",
      "Epoch 183/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7078e-04\n",
      "Epoch 184/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.6214e-04\n",
      "Epoch 185/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.9255e-04: 0s - loss: 1.9601e-0\n",
      "Epoch 186/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 2.1199e-04\n",
      "Epoch 187/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.5756e-04\n",
      "Epoch 188/300\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 1.7451e-04\n",
      "Epoch 189/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4996e-04\n",
      "Epoch 190/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4435e-04\n",
      "Epoch 191/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5921e-04\n",
      "Epoch 192/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.0009e-04\n",
      "Epoch 193/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.9084e-04\n",
      "Epoch 194/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7345e-04\n",
      "Epoch 195/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6243e-04\n",
      "Epoch 196/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5602e-04\n",
      "Epoch 197/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4187e-04\n",
      "Epoch 198/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.6179e-04\n",
      "Epoch 199/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4009e-04\n",
      "Epoch 200/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4910e-04\n",
      "Epoch 201/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6535e-04\n",
      "Epoch 202/300\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.7413e-0 - 0s 11ms/step - loss: 1.7349e-04\n",
      "Epoch 203/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.3138e-04\n",
      "Epoch 204/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6217e-04\n",
      "Epoch 205/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.2392e-04\n",
      "Epoch 206/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6621e-04\n",
      "Epoch 207/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6947e-04\n",
      "Epoch 208/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.5208e-04\n",
      "Epoch 209/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.2991e-04\n",
      "Epoch 210/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5832e-04\n",
      "Epoch 211/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.5220e-04\n",
      "Epoch 212/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5669e-04\n",
      "Epoch 213/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.4909e-04\n",
      "Epoch 214/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.6832e-04\n",
      "Epoch 215/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.6136e-04\n",
      "Epoch 216/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.7797e-04\n",
      "Epoch 217/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.3603e-04\n",
      "Epoch 218/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.3961e-04\n",
      "Epoch 219/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.5255e-04\n",
      "Epoch 220/300\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 1.8460e-04\n",
      "Epoch 221/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.5884e-04\n",
      "Epoch 222/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.3005e-04: 0s - loss: \n",
      "Epoch 223/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.4348e-04\n",
      "Epoch 224/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.7109e-04\n",
      "Epoch 225/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.6306e-04\n",
      "Epoch 226/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.4939e-04\n",
      "Epoch 227/300\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 1.5370e-04\n",
      "Epoch 228/300\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 1.2434e-04\n",
      "Epoch 229/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.5832e-04\n",
      "Epoch 230/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.2038e-04\n",
      "Epoch 231/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.8304e-04\n",
      "Epoch 232/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.3258e-04: 0s - loss: 9.7\n",
      "Epoch 233/300\n",
      "42/42 [==============================] - ETA: 0s - loss: 1.6706e-0 - 0s 10ms/step - loss: 1.6692e-04\n",
      "Epoch 234/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.4187e-04\n",
      "Epoch 235/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.6766e-04\n",
      "Epoch 236/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.7666e-04\n",
      "Epoch 237/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.3835e-04\n",
      "Epoch 238/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.3679e-04\n",
      "Epoch 239/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.5401e-04\n",
      "Epoch 240/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4123e-04\n",
      "Epoch 241/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.3903e-04\n",
      "Epoch 242/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.5186e-04\n",
      "Epoch 243/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.5296e-04\n",
      "Epoch 244/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.5260e-04\n",
      "Epoch 245/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.6603e-04\n",
      "Epoch 246/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.4136e-04\n",
      "Epoch 247/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.5170e-04\n",
      "Epoch 248/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.5673e-04\n",
      "Epoch 249/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.6518e-04\n",
      "Epoch 250/300\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 1.5589e-04\n",
      "Epoch 251/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.4312e-04\n",
      "Epoch 252/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.5776e-04\n",
      "Epoch 253/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.5844e-04\n",
      "Epoch 254/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.7816e-04\n",
      "Epoch 255/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.7632e-04\n",
      "Epoch 256/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.7304e-04\n",
      "Epoch 257/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.8634e-04\n",
      "Epoch 258/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.5431e-04\n",
      "Epoch 259/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.3674e-04\n",
      "Epoch 260/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.4643e-04\n",
      "Epoch 261/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.7248e-04\n",
      "Epoch 262/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.4398e-04\n",
      "Epoch 263/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.7702e-04\n",
      "Epoch 264/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.4468e-04\n",
      "Epoch 265/300\n",
      "42/42 [==============================] - 0s 10ms/step - loss: 1.8898e-04\n",
      "Epoch 266/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.9012e-04\n",
      "Epoch 267/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.5696e-04\n",
      "Epoch 268/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.7130e-04\n",
      "Epoch 269/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.6032e-04\n",
      "Epoch 270/300\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 1.7417e-04\n",
      "Epoch 271/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.3332e-04\n",
      "Epoch 272/300\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 1.3332e-04\n",
      "Epoch 273/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5216e-04\n",
      "Epoch 274/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.7076e-04\n",
      "Epoch 275/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.3160e-04\n",
      "Epoch 276/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.5861e-04\n",
      "Epoch 277/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.4791e-04\n",
      "Epoch 278/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5557e-04\n",
      "Epoch 279/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.2711e-04\n",
      "Epoch 280/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.4265e-04\n",
      "Epoch 281/300\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 1.3591e-04\n",
      "Epoch 282/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 1s 13ms/step - loss: 1.3611e-04\n",
      "Epoch 283/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.5758e-04\n",
      "Epoch 284/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.1978e-04\n",
      "Epoch 285/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.6850e-04\n",
      "Epoch 286/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.5266e-04\n",
      "Epoch 287/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.4174e-04\n",
      "Epoch 288/300\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 1.8884e-04\n",
      "Epoch 289/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.9451e-04\n",
      "Epoch 290/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.5409e-04\n",
      "Epoch 291/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.3416e-04\n",
      "Epoch 292/300\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 1.2921e-04\n",
      "Epoch 293/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.3080e-04\n",
      "Epoch 294/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.4382e-04\n",
      "Epoch 295/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.4353e-04\n",
      "Epoch 296/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.3711e-04\n",
      "Epoch 297/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.7267e-04\n",
      "Epoch 298/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.3398e-04\n",
      "Epoch 299/300\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 1.3880e-04\n",
      "Epoch 300/300\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 1.3714e-04\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(x_train,y_train,epochs=300,batch_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x270b58e9cd0>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>1465.199951</td>\n",
       "      <td>1553.359985</td>\n",
       "      <td>1460.930054</td>\n",
       "      <td>1539.130005</td>\n",
       "      <td>1539.130005</td>\n",
       "      <td>7983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>1520.010010</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1497.109985</td>\n",
       "      <td>1500.280029</td>\n",
       "      <td>1500.280029</td>\n",
       "      <td>6975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1594.000000</td>\n",
       "      <td>1518.310059</td>\n",
       "      <td>1575.390015</td>\n",
       "      <td>1575.390015</td>\n",
       "      <td>9182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>1602.310059</td>\n",
       "      <td>1634.560059</td>\n",
       "      <td>1589.189941</td>\n",
       "      <td>1629.510010</td>\n",
       "      <td>1629.510010</td>\n",
       "      <td>7993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>1664.689941</td>\n",
       "      <td>1676.609985</td>\n",
       "      <td>1616.609985</td>\n",
       "      <td>1656.579956</td>\n",
       "      <td>1656.579956</td>\n",
       "      <td>8881400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2019-01-02  1465.199951  1553.359985  1460.930054  1539.130005  1539.130005   \n",
       "2019-01-03  1520.010010  1538.000000  1497.109985  1500.280029  1500.280029   \n",
       "2019-01-04  1530.000000  1594.000000  1518.310059  1575.390015  1575.390015   \n",
       "2019-01-07  1602.310059  1634.560059  1589.189941  1629.510010  1629.510010   \n",
       "2019-01-08  1664.689941  1676.609985  1616.609985  1656.579956  1656.579956   \n",
       "\n",
       "             Volume  \n",
       "Date                 \n",
       "2019-01-02  7983100  \n",
       "2019-01-03  6975600  \n",
       "2019-01-04  9182600  \n",
       "2019-01-07  7993200  \n",
       "2019-01-08  8881400  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv(\"C:\\\\Users\\\\Krishna\\\\Desktop\\\\dataset\\\\AMZNtest.csv\",parse_dates=[\"Date\"],index_col=\"Date\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>1465.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>1520.010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>1530.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>1602.310059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>1664.689941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>1652.979980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>1641.010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>1640.550049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>1615.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>1632.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>1684.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>1680.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>1712.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>1681.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>1656.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>1641.069946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>1670.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>1643.589966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>1631.270020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>1623.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>1692.849976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open\n",
       "Date                   \n",
       "2019-01-02  1465.199951\n",
       "2019-01-03  1520.010010\n",
       "2019-01-04  1530.000000\n",
       "2019-01-07  1602.310059\n",
       "2019-01-08  1664.689941\n",
       "2019-01-09  1652.979980\n",
       "2019-01-10  1641.010010\n",
       "2019-01-11  1640.550049\n",
       "2019-01-14  1615.000000\n",
       "2019-01-15  1632.000000\n",
       "2019-01-16  1684.219971\n",
       "2019-01-17  1680.000000\n",
       "2019-01-18  1712.000000\n",
       "2019-01-22  1681.000000\n",
       "2019-01-23  1656.000000\n",
       "2019-01-24  1641.069946\n",
       "2019-01-25  1670.500000\n",
       "2019-01-28  1643.589966\n",
       "2019-01-29  1631.270020\n",
       "2019-01-30  1623.000000\n",
       "2019-01-31  1692.849976"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest=test[[\"Open\"]]\n",
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_xtest=sc.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.22208285],\n",
       "       [0.26256092],\n",
       "       [0.55555138],\n",
       "       [0.80830612],\n",
       "       [0.76085896],\n",
       "       [0.71235828],\n",
       "       [0.71049458],\n",
       "       [0.60696928],\n",
       "       [0.67585096],\n",
       "       [0.88743913],\n",
       "       [0.87034038],\n",
       "       [1.        ],\n",
       "       [0.87439225],\n",
       "       [0.77309567],\n",
       "       [0.71260113],\n",
       "       [0.83184768],\n",
       "       [0.72281191],\n",
       "       [0.67289318],\n",
       "       [0.63938419],\n",
       "       [0.92240673]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc_xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test=feature_transform(n_steps,sc_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test.reshape((x_test.shape[0],x_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Do the prediction and check performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.797676  ],\n",
       "       [0.8818997 ],\n",
       "       [0.86041373],\n",
       "       [0.77647364],\n",
       "       [0.71124893],\n",
       "       [0.7958966 ],\n",
       "       [0.7276711 ],\n",
       "       [0.6757051 ],\n",
       "       [0.64433056]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=sc.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1662.0664],\n",
       "       [1682.8529],\n",
       "       [1677.55  ],\n",
       "       [1656.8336],\n",
       "       [1640.7362],\n",
       "       [1661.6272],\n",
       "       [1644.7892],\n",
       "       [1631.964 ],\n",
       "       [1624.2207]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=sc.inverse_transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1712.      ],\n",
       "       [1681.      ],\n",
       "       [1656.      ],\n",
       "       [1641.069946],\n",
       "       [1670.5     ],\n",
       "       [1643.589966],\n",
       "       [1631.27002 ],\n",
       "       [1623.      ],\n",
       "       [1692.849976]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RMSE performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is : 1043.7797953918057\n",
      "RMSE: 32.30758108233741\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(y_test.flatten(),prediction.flatten())\n",
    "print(\"MSE is :\",mse)\n",
    "print(\"RMSE:\",np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the GRU model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "9/9 [==============================] - 19s 91ms/step - loss: 0.1329\n",
      "Epoch 2/50\n",
      "9/9 [==============================] - 1s 99ms/step - loss: 0.0577\n",
      "Epoch 3/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0286\n",
      "Epoch 4/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0110\n",
      "Epoch 5/50\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 0.0055: 0s - loss: 0.0\n",
      "Epoch 6/50\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.0035\n",
      "Epoch 7/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0036\n",
      "Epoch 8/50\n",
      "9/9 [==============================] - 1s 92ms/step - loss: 0.0032\n",
      "Epoch 9/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0030\n",
      "Epoch 10/50\n",
      "9/9 [==============================] - 1s 105ms/step - loss: 0.0027\n",
      "Epoch 11/50\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 0.0030\n",
      "Epoch 12/50\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0026\n",
      "Epoch 13/50\n",
      "9/9 [==============================] - 1s 116ms/step - loss: 0.0025\n",
      "Epoch 14/50\n",
      "9/9 [==============================] - 1s 112ms/step - loss: 0.0027\n",
      "Epoch 15/50\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0024\n",
      "Epoch 16/50\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.0025\n",
      "Epoch 17/50\n",
      "9/9 [==============================] - 1s 85ms/step - loss: 0.0022\n",
      "Epoch 18/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0025\n",
      "Epoch 19/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0025\n",
      "Epoch 20/50\n",
      "9/9 [==============================] - 1s 82ms/step - loss: 0.0021\n",
      "Epoch 21/50\n",
      "9/9 [==============================] - 1s 84ms/step - loss: 0.0022\n",
      "Epoch 22/50\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.0022\n",
      "Epoch 23/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0023\n",
      "Epoch 24/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0024\n",
      "Epoch 25/50\n",
      "9/9 [==============================] - 1s 113ms/step - loss: 0.0023\n",
      "Epoch 26/50\n",
      "9/9 [==============================] - 1s 118ms/step - loss: 0.0021\n",
      "Epoch 27/50\n",
      "9/9 [==============================] - 1s 108ms/step - loss: 0.0027\n",
      "Epoch 28/50\n",
      "9/9 [==============================] - 1s 102ms/step - loss: 0.0019\n",
      "Epoch 29/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0019\n",
      "Epoch 30/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0020\n",
      "Epoch 31/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0026\n",
      "Epoch 32/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0024\n",
      "Epoch 33/50\n",
      "9/9 [==============================] - 1s 90ms/step - loss: 0.0021\n",
      "Epoch 34/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0022\n",
      "Epoch 35/50\n",
      "9/9 [==============================] - 1s 98ms/step - loss: 0.0022\n",
      "Epoch 36/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0022\n",
      "Epoch 37/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0021\n",
      "Epoch 38/50\n",
      "9/9 [==============================] - 1s 106ms/step - loss: 0.0021\n",
      "Epoch 39/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0020\n",
      "Epoch 40/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0024\n",
      "Epoch 41/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0024\n",
      "Epoch 42/50\n",
      "9/9 [==============================] - 1s 95ms/step - loss: 0.0017\n",
      "Epoch 43/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0019\n",
      "Epoch 44/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0018\n",
      "Epoch 45/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0021\n",
      "Epoch 46/50\n",
      "9/9 [==============================] - 1s 94ms/step - loss: 0.0021\n",
      "Epoch 47/50\n",
      "9/9 [==============================] - 1s 91ms/step - loss: 0.0023\n",
      "Epoch 48/50\n",
      "9/9 [==============================] - 1s 93ms/step - loss: 0.0019\n",
      "Epoch 49/50\n",
      "9/9 [==============================] - 1s 97ms/step - loss: 0.0020\n",
      "Epoch 50/50\n",
      "9/9 [==============================] - 1s 96ms/step - loss: 0.0021\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dropout, GRU, Bidirectional\n",
    "regressorGRU = Sequential()\n",
    "# First GRU layer with Dropout regularisation\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(x_train.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Second GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(x_train.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Third GRU layer\n",
    "regressorGRU.add(GRU(units=50, return_sequences=True, input_shape=(x_train.shape[1],1), activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# Fourth GRU layer\n",
    "regressorGRU.add(GRU(units=50, activation='tanh'))\n",
    "regressorGRU.add(Dropout(0.2))\n",
    "# The output layer\n",
    "regressorGRU.add(Dense(units=1))\n",
    "# Compiling the RNN\n",
    "regressorGRU.compile(optimizer=SGD(lr=0.01, decay=1e-7, momentum=0.9, nesterov=False),loss='mean_squared_error')\n",
    "# Fitting to the training set\n",
    "history=regressorGRU.fit(x_train,y_train,epochs=50,batch_size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x270c2e754c0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>1465.199951</td>\n",
       "      <td>1553.359985</td>\n",
       "      <td>1460.930054</td>\n",
       "      <td>1539.130005</td>\n",
       "      <td>1539.130005</td>\n",
       "      <td>7983100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>1520.010010</td>\n",
       "      <td>1538.000000</td>\n",
       "      <td>1497.109985</td>\n",
       "      <td>1500.280029</td>\n",
       "      <td>1500.280029</td>\n",
       "      <td>6975600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1594.000000</td>\n",
       "      <td>1518.310059</td>\n",
       "      <td>1575.390015</td>\n",
       "      <td>1575.390015</td>\n",
       "      <td>9182600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>1602.310059</td>\n",
       "      <td>1634.560059</td>\n",
       "      <td>1589.189941</td>\n",
       "      <td>1629.510010</td>\n",
       "      <td>1629.510010</td>\n",
       "      <td>7993200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>1664.689941</td>\n",
       "      <td>1676.609985</td>\n",
       "      <td>1616.609985</td>\n",
       "      <td>1656.579956</td>\n",
       "      <td>1656.579956</td>\n",
       "      <td>8881400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open         High          Low        Close    Adj Close  \\\n",
       "Date                                                                          \n",
       "2019-01-02  1465.199951  1553.359985  1460.930054  1539.130005  1539.130005   \n",
       "2019-01-03  1520.010010  1538.000000  1497.109985  1500.280029  1500.280029   \n",
       "2019-01-04  1530.000000  1594.000000  1518.310059  1575.390015  1575.390015   \n",
       "2019-01-07  1602.310059  1634.560059  1589.189941  1629.510010  1629.510010   \n",
       "2019-01-08  1664.689941  1676.609985  1616.609985  1656.579956  1656.579956   \n",
       "\n",
       "             Volume  \n",
       "Date                 \n",
       "2019-01-02  7983100  \n",
       "2019-01-03  6975600  \n",
       "2019-01-04  9182600  \n",
       "2019-01-07  7993200  \n",
       "2019-01-08  8881400  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=pd.read_csv(\"C:\\\\Users\\\\Krishna\\\\Desktop\\\\dataset\\\\AMZNtest.csv\",parse_dates=[\"Date\"],index_col=\"Date\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-02</th>\n",
       "      <td>1465.199951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-03</th>\n",
       "      <td>1520.010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-04</th>\n",
       "      <td>1530.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-07</th>\n",
       "      <td>1602.310059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-08</th>\n",
       "      <td>1664.689941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-09</th>\n",
       "      <td>1652.979980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-10</th>\n",
       "      <td>1641.010010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-11</th>\n",
       "      <td>1640.550049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-14</th>\n",
       "      <td>1615.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-15</th>\n",
       "      <td>1632.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-16</th>\n",
       "      <td>1684.219971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-17</th>\n",
       "      <td>1680.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-18</th>\n",
       "      <td>1712.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-22</th>\n",
       "      <td>1681.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-23</th>\n",
       "      <td>1656.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-24</th>\n",
       "      <td>1641.069946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-25</th>\n",
       "      <td>1670.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-28</th>\n",
       "      <td>1643.589966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-29</th>\n",
       "      <td>1631.270020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-30</th>\n",
       "      <td>1623.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-31</th>\n",
       "      <td>1692.849976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Open\n",
       "Date                   \n",
       "2019-01-02  1465.199951\n",
       "2019-01-03  1520.010010\n",
       "2019-01-04  1530.000000\n",
       "2019-01-07  1602.310059\n",
       "2019-01-08  1664.689941\n",
       "2019-01-09  1652.979980\n",
       "2019-01-10  1641.010010\n",
       "2019-01-11  1640.550049\n",
       "2019-01-14  1615.000000\n",
       "2019-01-15  1632.000000\n",
       "2019-01-16  1684.219971\n",
       "2019-01-17  1680.000000\n",
       "2019-01-18  1712.000000\n",
       "2019-01-22  1681.000000\n",
       "2019-01-23  1656.000000\n",
       "2019-01-24  1641.069946\n",
       "2019-01-25  1670.500000\n",
       "2019-01-28  1643.589966\n",
       "2019-01-29  1631.270020\n",
       "2019-01-30  1623.000000\n",
       "2019-01-31  1692.849976"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest=test[[\"Open\"]]\n",
    "xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_xtest=sc.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test,y_test=feature_transform(n_steps,sc_xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x_test.reshape((x_test.shape[0],x_test.shape[1],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets Do the prediction and check performance metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=regressorGRU.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6665255 ],\n",
       "       [0.715446  ],\n",
       "       [0.75867164],\n",
       "       [0.79069126],\n",
       "       [0.803403  ],\n",
       "       [0.80500734],\n",
       "       [0.8008125 ],\n",
       "       [0.789858  ],\n",
       "       [0.7718718 ]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=sc.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=sc.inverse_transform(y_test.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate RMSE performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error,r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE is : 1434.8222568365375\n",
      "RMSE: 37.87904772874494\n"
     ]
    }
   ],
   "source": [
    "mse=mean_squared_error(y_test.flatten(),prediction.flatten())\n",
    "print(\"MSE is :\",mse)\n",
    "print(\"RMSE:\",np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
